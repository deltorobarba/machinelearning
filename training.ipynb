{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deltorobarba/machinelearning/blob/main/training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Create a custom-trained model from a Python script in a Docker container using the Vertex AI SDK for Python, and then get a prediction from the deployed model by sending data.*"
      ],
      "metadata": {
        "id": "xadJhr_FwsSI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dataset:custom,cifar10,icn"
      },
      "source": [
        "The dataset used for this tutorial is the penguins dataset from [BigQuery public datasets](https://cloud.google.com/bigquery/public-data). For this tutorial, you use only the fields `culmen_length_mm`, `culmen_depth_mm`, `flipper_length_mm`, `body_mass_g` from the dataset to predict the penguins species (`species`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fd00fa70a2a",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Install the packages\n",
        "! pip3 install --upgrade google-cloud-aiplatform \\\n",
        "                        google-cloud-storage \\\n",
        "                        'google-cloud-bigquery[pandas]'\n",
        "\n",
        "#automatically restarts kernel\n",
        "import IPython\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3c8049930470",
        "tags": [],
        "outputId": "34a00048-e41e-419a-bc4d-18d01082deb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated property [core/project].\n"
          ]
        }
      ],
      "source": [
        "PROJECT=!(gcloud config get-value project)\n",
        "PROJECT_ID=\"qwiklabs-gcp-00-34bdec36e87f\"\n",
        "\n",
        "# Set the project id\n",
        "! gcloud config set project {PROJECT_ID}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3aaadaaf9b30",
        "tags": []
      },
      "outputs": [],
      "source": [
        "REGION = \"us-west1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bucket",
        "tags": []
      },
      "outputs": [],
      "source": [
        "BUCKET_NAME = \"qwiklabs-gcp-00-34bdec36e87f-cymbal\" # update it from the lab instructions\n",
        "BUCKET_URI = f\"gs://{BUCKET_NAME}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9d3ac73dfbc",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from google.cloud import aiplatform\n",
        "\n",
        "# Initialize the Vertex AI SDK\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fad2ba1ad7c3",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from google.cloud import bigquery\n",
        "\n",
        "# Set up BigQuery client\n",
        "bq_client = bigquery.Client(project=PROJECT_ID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3a2449cfcf1",
        "tags": [],
        "outputId": "10e2977c-652d-4fc9-95fe-d6c9f8ecaf6d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/envs/tensorflow/lib/python3.10/site-packages/google/cloud/bigquery/table.py:2309: UserWarning: Unable to represent RANGE schema as struct using pandas ArrowDtype. Using `object` instead. To use ArrowDtype, use pandas >= 1.5 and pyarrow >= 10.0.1.\n",
            "  warnings.warn(_RANGE_PYARROW_WARNING)\n",
            "/opt/conda/envs/tensorflow/lib/python3.10/site-packages/google/cloud/bigquery/table.py:2323: UserWarning: Unable to represent RANGE schema as struct using pandas ArrowDtype. Using `object` instead. To use ArrowDtype, use pandas >= 1.5 and pyarrow >= 10.0.1.\n",
            "  warnings.warn(_RANGE_PYARROW_WARNING)\n",
            "/opt/conda/envs/tensorflow/lib/python3.10/site-packages/google/cloud/bigquery/table.py:2337: UserWarning: Unable to represent RANGE schema as struct using pandas ArrowDtype. Using `object` instead. To use ArrowDtype, use pandas >= 1.5 and pyarrow >= 10.0.1.\n",
            "  warnings.warn(_RANGE_PYARROW_WARNING)\n"
          ]
        }
      ],
      "source": [
        "# Create a Vertex AI Tabular Dataset from the BigQuery dataset\n",
        "# Preprocess data and split data: Convert categorical features to numeric\n",
        "# Split train and test data in the fration 80-20 ratio\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "LABEL_COLUMN = \"species\"\n",
        "\n",
        "# Define the BigQuery source dataset\n",
        "BQ_SOURCE = \"bigquery-public-data.ml_datasets.penguins\"\n",
        "\n",
        "# Define NA values\n",
        "NA_VALUES = [\"NA\", \".\"]\n",
        "\n",
        "# Download a table\n",
        "table = bq_client.get_table(BQ_SOURCE)\n",
        "df = bq_client.list_rows(table).to_dataframe()\n",
        "\n",
        "# Drop unusable rows\n",
        "df = df.replace(to_replace=NA_VALUES, value=np.NaN).dropna()\n",
        "\n",
        "# Convert categorical columns to numeric\n",
        "df[\"island\"], _ = pd.factorize(df[\"island\"])\n",
        "df[\"species\"], _ = pd.factorize(df[\"species\"])\n",
        "df[\"sex\"], _ = pd.factorize(df[\"sex\"])\n",
        "\n",
        "# Split into a training and holdout dataset\n",
        "df_train = df.sample(frac=0.8, random_state=100)\n",
        "df_holdout = df[~df.index.isin(df_train.index)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7fa452ee5c75",
        "tags": [],
        "outputId": "49704300-1956-4d13-af13-e155ba6c3047"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset(DatasetReference('qwiklabs-gcp-00-34bdec36e87f', 'cymbal_penguins_dataset'))"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create BigQuery dataset\n",
        "BQ_DATASET=\"cymbal_penguins_dataset\"\n",
        "bq_dataset_id = f\"{PROJECT_ID}.{BQ_DATASET}\"\n",
        "bq_dataset = bigquery.Dataset(bq_dataset_id)\n",
        "bq_client.create_dataset(bq_dataset, exists_ok=True)\n",
        "\n",
        "# Create a Vertex AI tabular dataset from BigQuery training data\n",
        "#df_source=df_train\n",
        "#staging_path=table name provided in lab instructions\n",
        "#display_name=as provided in the lab instructions\n",
        "\n",
        "#[ TODO - Insert your code ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "n0oUbGzJsDSX",
        "outputId": "488ae63e-d0c4-48bf-a388-c96f05619a35"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>species</th>\n",
              "      <th>island</th>\n",
              "      <th>culmen_length_mm</th>\n",
              "      <th>culmen_depth_mm</th>\n",
              "      <th>flipper_length_mm</th>\n",
              "      <th>body_mass_g</th>\n",
              "      <th>sex</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>183</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>50.2</td>\n",
              "      <td>14.3</td>\n",
              "      <td>218.0</td>\n",
              "      <td>5700.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>180</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>48.2</td>\n",
              "      <td>14.3</td>\n",
              "      <td>210.0</td>\n",
              "      <td>4600.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>340</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>40.9</td>\n",
              "      <td>16.8</td>\n",
              "      <td>191.0</td>\n",
              "      <td>3700.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>46.2</td>\n",
              "      <td>17.5</td>\n",
              "      <td>187.0</td>\n",
              "      <td>3650.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>295</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>37.2</td>\n",
              "      <td>19.4</td>\n",
              "      <td>184.0</td>\n",
              "      <td>3900.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>41.1</td>\n",
              "      <td>19.0</td>\n",
              "      <td>182.0</td>\n",
              "      <td>3425.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>45.3</td>\n",
              "      <td>13.7</td>\n",
              "      <td>210.0</td>\n",
              "      <td>4300.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>337</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>41.8</td>\n",
              "      <td>19.4</td>\n",
              "      <td>198.0</td>\n",
              "      <td>4450.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>36.5</td>\n",
              "      <td>18.0</td>\n",
              "      <td>182.0</td>\n",
              "      <td>3150.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>219</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>45.5</td>\n",
              "      <td>15.0</td>\n",
              "      <td>220.0</td>\n",
              "      <td>5000.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>266 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     species  island  culmen_length_mm  culmen_depth_mm  flipper_length_mm  \\\n",
              "183        2       1              50.2             14.3              218.0   \n",
              "180        2       1              48.2             14.3              210.0   \n",
              "340        0       2              40.9             16.8              191.0   \n",
              "47         1       0              46.2             17.5              187.0   \n",
              "295        0       2              37.2             19.4              184.0   \n",
              "..       ...     ...               ...              ...                ...   \n",
              "96         0       0              41.1             19.0              182.0   \n",
              "175        2       1              45.3             13.7              210.0   \n",
              "337        0       2              41.8             19.4              198.0   \n",
              "95         0       0              36.5             18.0              182.0   \n",
              "219        2       1              45.5             15.0              220.0   \n",
              "\n",
              "     body_mass_g  sex  \n",
              "183       5700.0    1  \n",
              "180       4600.0    0  \n",
              "340       3700.0    0  \n",
              "47        3650.0    0  \n",
              "295       3900.0    1  \n",
              "..           ...  ...  \n",
              "96        3425.0    1  \n",
              "175       4300.0    0  \n",
              "337       4450.0    1  \n",
              "95        3150.0    0  \n",
              "219       5000.0    1  \n",
              "\n",
              "[266 rows x 7 columns]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "5m5msEVUsDSX",
        "outputId": "575c1717-1aa7-48fe-926a-ae8bf91dc5f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your DataFrame has 266 rows and AutoML requires 1000 rows to train on tabular data. You can still train a custom model once your dataset has been uploaded to Vertex, but you will not be able to use AutoML for training.\n",
            "Creating TabularDataset\n",
            "Create TabularDataset backing LRO: projects/518128629599/locations/us-west1/datasets/2324358785025441792/operations/1641925452769525760\n",
            "TabularDataset created. Resource name: projects/518128629599/locations/us-west1/datasets/2324358785025441792\n",
            "To use this TabularDataset in another session:\n",
            "ds = aiplatform.TabularDataset('projects/518128629599/locations/us-west1/datasets/2324358785025441792')\n"
          ]
        }
      ],
      "source": [
        "# Create a Vertex AI tabular dataset\n",
        "dataset = aiplatform.TabularDataset.create_from_dataframe(\n",
        "    df_source=df_train,\n",
        "    staging_path=f\"bq://qwiklabs-gcp-00-34bdec36e87f.cymbal_penguins_dataset.cymbal_penguins_table\",\n",
        "    display_name=\"cymbal_penguins\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "_HRvA-XrsDSX"
      },
      "outputs": [],
      "source": [
        "from google.cloud import aiplatform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "LXJXJJx_sDSX"
      },
      "outputs": [],
      "source": [
        "aiplatform.init(project=\"qwiklabs-gcp-00-34bdec36e87f\", location=\"us-west1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "G4GAxBrVsDSX"
      },
      "outputs": [],
      "source": [
        "# bq_source = \"bq://qwiklabs-gcp-00-34bdec36e87f.cymbal_penguins.cymbal_penguins_table\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1npiDcUtlugw",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Train Model. Define the command args for the training script\n",
        "\n",
        "EPOCHS = 20\n",
        "BATCH_SIZE = 10\n",
        "\n",
        "CMDARGS = [\n",
        "    \"--label_column=\" + LABEL_COLUMN,\n",
        "    \"--epochs=\" + str(EPOCHS),\n",
        "    \"--batch_size=\" + str(BATCH_SIZE),\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taskpy_contents"
      },
      "source": [
        "Training script: Complete the contents of the training script, `task.py`. You need to write code in the **[ TODO - Insert your code ]** section by training the model with epochs and batch size according and saves the trained model artifact to Cloud Storage directory `aiplatform-custom-training` in the created Cloud Storage Bucket location using `os.environ['AIP_MODEL_DIR']`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72rUqXNFlugx",
        "tags": [],
        "outputId": "96334e50-2087-4276-939b-73cddd73c679"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting task.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile task.py\n",
        "\n",
        "import argparse\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from google.cloud import bigquery\n",
        "from google.cloud import storage\n",
        "\n",
        "# Read environmental variables\n",
        "training_data_uri = os.getenv(\"AIP_TRAINING_DATA_URI\")\n",
        "validation_data_uri = os.getenv(\"AIP_VALIDATION_DATA_URI\")\n",
        "test_data_uri = os.getenv(\"AIP_TEST_DATA_URI\")\n",
        "\n",
        "# Read args\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--label_column', required=True, type=str)\n",
        "parser.add_argument('--epochs', default=10, type=int)\n",
        "parser.add_argument('--batch_size', default=10, type=int)\n",
        "args = parser.parse_args()\n",
        "\n",
        "# Set up training variables\n",
        "LABEL_COLUMN = args.label_column\n",
        "\n",
        "# See https://cloud.google.com/vertex-ai/docs/workbench/managed/executor#explicit-project-selection for issues regarding permissions.\n",
        "PROJECT_NUMBER = os.environ[\"CLOUD_ML_PROJECT_ID\"]\n",
        "bq_client = bigquery.Client(project=PROJECT_NUMBER)\n",
        "\n",
        "\n",
        "# Download a table\n",
        "def download_table(bq_table_uri: str):\n",
        "    # Remove bq:// prefix if present\n",
        "    prefix = \"bq://\"\n",
        "    if bq_table_uri.startswith(prefix):\n",
        "        bq_table_uri = bq_table_uri[len(prefix) :]\n",
        "\n",
        "    # Download the BigQuery table as a dataframe\n",
        "    # This requires the \"BigQuery Read Session User\" role on the custom training service account.\n",
        "    table = bq_client.get_table(bq_table_uri)\n",
        "    return bq_client.list_rows(table).to_dataframe()\n",
        "\n",
        "# Download dataset splits\n",
        "df_train = download_table(training_data_uri)\n",
        "df_validation = download_table(validation_data_uri)\n",
        "df_test = download_table(test_data_uri)\n",
        "\n",
        "def convert_dataframe_to_dataset(\n",
        "    df_train: pd.DataFrame,\n",
        "    df_validation: pd.DataFrame,\n",
        "):\n",
        "    df_train_x, df_train_y = df_train, df_train.pop(LABEL_COLUMN)\n",
        "    df_validation_x, df_validation_y = df_validation, df_validation.pop(LABEL_COLUMN)\n",
        "\n",
        "    y_train = tf.convert_to_tensor(np.asarray(df_train_y).astype(\"float32\"))\n",
        "    y_validation = tf.convert_to_tensor(np.asarray(df_validation_y).astype(\"float32\"))\n",
        "\n",
        "    # Convert to numpy representation\n",
        "    x_train = tf.convert_to_tensor(np.asarray(df_train_x).astype(\"float32\"))\n",
        "    x_test = tf.convert_to_tensor(np.asarray(df_validation_x).astype(\"float32\"))\n",
        "\n",
        "    # Convert to one-hot representation\n",
        "    num_species = len(df_train_y.unique())\n",
        "    y_train = tf.keras.utils.to_categorical(y_train, num_classes=num_species)\n",
        "    y_validation = tf.keras.utils.to_categorical(y_validation, num_classes=num_species)\n",
        "\n",
        "    dataset_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "    dataset_validation = tf.data.Dataset.from_tensor_slices((x_test, y_validation))\n",
        "    return (dataset_train, dataset_validation)\n",
        "\n",
        "# Create datasets\n",
        "dataset_train, dataset_validation = convert_dataframe_to_dataset(df_train, df_validation)\n",
        "\n",
        "# Shuffle train set\n",
        "dataset_train = dataset_train.shuffle(len(df_train))\n",
        "\n",
        "def create_model(num_features):\n",
        "    # Create model\n",
        "    Dense = tf.keras.layers.Dense\n",
        "    model = tf.keras.Sequential(\n",
        "        [\n",
        "            Dense(\n",
        "                100,\n",
        "                activation=tf.nn.relu,\n",
        "                kernel_initializer=\"uniform\",\n",
        "                input_dim=num_features,\n",
        "            ),\n",
        "            Dense(75, activation=tf.nn.relu),\n",
        "            Dense(50, activation=tf.nn.relu),\n",
        "            Dense(25, activation=tf.nn.relu),\n",
        "            Dense(3, activation=tf.nn.softmax),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Compile Keras model\n",
        "    optimizer = tf.keras.optimizers.RMSprop(lr=0.001)\n",
        "    model.compile(\n",
        "        loss=\"categorical_crossentropy\", metrics=[\"accuracy\"], optimizer=optimizer\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# Create the model\n",
        "model = create_model(num_features=dataset_train._flat_shapes[0].dims[0].value)\n",
        "\n",
        "# Set up datasets\n",
        "dataset_train = dataset_train.batch(args.batch_size)\n",
        "dataset_validation = dataset_validation.batch(args.batch_size)\n",
        "\n",
        "# Train the model\n",
        "model.fit(dataset_train, epochs=args.epochs, validation_data=dataset_validation)\n",
        "\n",
        "tf.saved_model.save(model, os.getenv(\"AIP_MODEL_DIR\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "train_custom_job"
      },
      "source": [
        "Executes script in Cloud Vertex AI Training Define your custom `TrainingPipeline` on Vertex AI.\n",
        "\n",
        "Use the `CustomTrainingJob` class to define the `TrainingPipeline`. The class takes the following parameters:\n",
        "\n",
        "- `display_name`: The user-defined name of this training pipeline.\n",
        "- `script_path`: The local path to the training script.\n",
        "- `container_uri`: The URI of the training container image.\n",
        "- `requirements`: The list of Python package dependencies of the script.\n",
        "- `model_serving_container_image_uri`: The URI of a container that can serve predictions for your model — either a pre-built container or a custom container.\n",
        "\n",
        "Use the `run` function to start training.\n",
        "\n",
        "The `run` function creates a training pipeline that trains and creates a `Model` object. After the training pipeline completes, the `run` function returns the `Model` object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxIxvDdglugx",
        "tags": [],
        "outputId": "03cc07b9-1ab5-42e1-df25-533adf7602da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training script copied to:\n",
            "gs://qwiklabs-gcp-00-34bdec36e87f-cymbal/aiplatform-2024-11-01-15:50:23.716-aiplatform_custom_trainer_script-0.1.tar.gz.\n",
            "Training Output directory:\n",
            "gs://qwiklabs-gcp-00-34bdec36e87f-cymbal/aiplatform-custom-training-2024-11-01-15:50:23.798 \n",
            "No dataset split provided. The service will use a default split.\n",
            "View Training:\n",
            "https://console.cloud.google.com/ai/platform/locations/us-west1/training/5393797776322592768?project=518128629599\n",
            "CustomTrainingJob projects/518128629599/locations/us-west1/trainingPipelines/5393797776322592768 current state:\n",
            "PipelineState.PIPELINE_STATE_PENDING\n",
            "CustomTrainingJob projects/518128629599/locations/us-west1/trainingPipelines/5393797776322592768 current state:\n",
            "PipelineState.PIPELINE_STATE_PENDING\n",
            "CustomTrainingJob projects/518128629599/locations/us-west1/trainingPipelines/5393797776322592768 current state:\n",
            "PipelineState.PIPELINE_STATE_PENDING\n",
            "CustomTrainingJob projects/518128629599/locations/us-west1/trainingPipelines/5393797776322592768 current state:\n",
            "PipelineState.PIPELINE_STATE_PENDING\n",
            "CustomTrainingJob projects/518128629599/locations/us-west1/trainingPipelines/5393797776322592768 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n"
          ]
        }
      ],
      "source": [
        "JOB_NAME = \"cymbal_custom_training_job\"\n",
        "MODEL_DISPLAY_NAME = \"cymbal_penguins_model\"\n",
        "\n",
        "\n",
        "# Use the `CustomTrainingJob` class to define the `TrainingPipeline`.\n",
        "# container_uri=\"us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-8:latest\"\n",
        "# requirements=[\"google-cloud-bigquery[pandas]\", \"protobuf<3.20.0\"]\n",
        "# model_serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-8:latest\",\n",
        "\n",
        "# Define the training pipeline\n",
        "job = aiplatform.CustomTrainingJob(\n",
        "    display_name=JOB_NAME,\n",
        "    script_path=\"task.py\",\n",
        "    container_uri=\"us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-8:latest\",\n",
        "    requirements=[\"google-cloud-bigquery[pandas]\", \"protobuf<3.20.0\"],\n",
        "    model_serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-8:latest\",\n",
        ")\n",
        "\n",
        "# Use the `run` function to start training\n",
        "\n",
        "model = job.run(\n",
        "    dataset=dataset,\n",
        "    model_display_name=MODEL_DISPLAY_NAME,\n",
        "    bigquery_destination=f\"bq://{PROJECT_ID}\",\n",
        "    args=CMDARGS,\n",
        ")\n",
        "\n",
        "# Run the training job"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WMH7GrYMlugy"
      },
      "outputs": [],
      "source": [
        "# Deploy the model\n",
        "# Create an Endpoint resource for deploying the Model resource to.\n",
        "# Deploy the Model resource to the Endpoint resource.\n",
        "\n",
        "DEPLOYED_NAME = \"penguins_deployed\"\n",
        "\n",
        "# Deploy the model at model endpoint\n",
        "[ TODO - Insert your code ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67aeea91384a"
      },
      "outputs": [],
      "source": [
        "# Process the test data and make an online prediction request\n",
        "# (Send an online prediction request to your deployed model)\n",
        "\n",
        "# Prepare test data by convert it to a Python list\n",
        "df_holdout_y = df_holdout.pop(LABEL_COLUMN)\n",
        "df_holdout_x = df_holdout\n",
        "\n",
        "# Convert to list representation\n",
        "holdout_x = np.array(df_holdout_x).tolist()\n",
        "holdout_y = np.array(df_holdout_y).astype(\"float32\").tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "send_prediction_request:image"
      },
      "source": [
        "Send the prediction request. Now that you have test data, you can use it to send a prediction request. Use the `Endpoint` object's `predict` function, which takes the following parameters:\n",
        "\n",
        "- `instances`: A list of penguin measurement instances. According to your custom model, each instance should be an array of numbers. You prepared this list in the previous step.\n",
        "\n",
        "The `predict` function returns a list, where each element in the list corresponds to the an instance in the request. In the output for each prediction, you see the following:\n",
        "\n",
        "- Confidence level for the prediction (`predictions`), between 0 and 1, for each of the ten classes.\n",
        "\n",
        "You can then run a quick evaluation on the prediction results:\n",
        "1. `np.argmax`: Convert each list of confidence levels to a label\n",
        "2. Print predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6e20473b09f5"
      },
      "outputs": [],
      "source": [
        "predictions = endpoint.predict(instances=holdout_x)\n",
        "y_predicted = np.argmax(predictions.predictions, axis=1)\n",
        "\n",
        "y_predicted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZRHjQ_adsDSZ"
      },
      "outputs": [],
      "source": [
        "def save_prediction_output(bucket_name, blob_name, predicted_output):\n",
        "    from google.cloud import storage\n",
        "\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blob = bucket.blob(blob_name)\n",
        "\n",
        "    with blob.open(\"w\") as f:\n",
        "        f.write(predicted_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VK8_gI4RsDSZ"
      },
      "outputs": [],
      "source": [
        "save_prediction_output(f\"{BUCKET_NAME}\", \"prediction.txt\", str(y_predicted))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "environment": {
      "kernel": "conda-env-tensorflow-tensorflow",
      "name": "workbench-notebooks.m125",
      "type": "gcloud",
      "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
    },
    "kernelspec": {
      "display_name": "TensorFlow 2-11 (Local)",
      "language": "python",
      "name": "conda-env-tensorflow-tensorflow"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}